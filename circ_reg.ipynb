{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ece7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43daf3",
   "metadata": {},
   "source": [
    "In these experiments, we try to use circular regression to solve the following problem:\n",
    "given pairs (a, b=a*s mod p), find s.\n",
    "\n",
    "First, we visualize the loss and gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f223e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(pred, a, b, p):\n",
    "    diff = 2 * np.pi / p * (b - a * pred)\n",
    "    loss = -np.sum(np.cos(diff))\n",
    "    grad = -2 * np.pi / p * np.sum(a * np.sin(diff))\n",
    "    return loss, grad\n",
    "\n",
    "def plot_examples(p, s):\n",
    "        a = np.arange(p)\n",
    "        b = (a*s) % p\n",
    "\n",
    "        x = [s-p//2 + 0.03*i for i in range(int(p/0.03))]\n",
    "        likelihoods = np.array([likelihood(x, a, b, p) for x in x])\n",
    "        losses, grads = likelihoods[:,0], likelihoods[:,1]\n",
    "\n",
    "        xi = [i for i in range(s-p//2, s)] + [i for i in range(s+1, s+p//2+1)]\n",
    "        likelihoodsi = np.array([likelihood(x, a, b, p) for x in xi])\n",
    "        _, gradsi = likelihoodsi[:,0], likelihoodsi[:,1]\n",
    "\n",
    "        _, axs = plt.subplots(1,3, figsize = (20, 5))\n",
    "        axs[0].plot(x, losses)\n",
    "        axs[0].set_ylabel('loss')\n",
    "        axs[0].set_xlabel('prediction')\n",
    "        axs[0].set_title(f'Circular Regression Loss, p={p}, s={s}')\n",
    "\n",
    "        axs[1].plot(x, grads)\n",
    "        axs[1].scatter(xi, gradsi, s=7, c='r')\n",
    "        axs[1].set_ylabel('gradient')\n",
    "        axs[1].set_xlabel('prediction')\n",
    "        axs[1].set_title(f'Circular Regression Gradient, p={p}, s={s}')\n",
    "        \n",
    "        axs[2].scatter(xi, 1/gradsi, s=5)\n",
    "        axs[2].set_ylabel('1 / gradient')\n",
    "        axs[2].set_xlabel('prediction')\n",
    "        axs[2].set_title(f'grad_r = 1 / gradient, p={p}, s={s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92994de2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p, s = 41, 3\n",
    "# for s in np.random.choice(p, 3):\n",
    "# for p in [23, 41, 71, 113, 251, 367, 967, 1471]:\n",
    "plot_examples(p, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ffe05",
   "metadata": {},
   "source": [
    "From the plots above, we conclude that the loss is lowest at the correct answers but much closer to 0 everywhere else. For simplicity, we only show one interval of length p. It's periodic. Although it has a local minimum in each interval of length p, all the local minima are the global minimum. The gradient at integer points always has the sign that points to the closest correct answer. \n",
    "\n",
    "However, the gradient's magnitude is giving the opposite information of how large a step we want to take. It's extremely large when it's close to the answer. So, instead of using a fixed learning rate multiplied on the gradient, we try taking the reciprocal of the gradient, implemented as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4bf6b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def circ_reg(p, A, B, lr, bs):\n",
    "    batch_size = min(len(A), bs)\n",
    "    # Augment the dataset if the original dataset is too small and not the full set\n",
    "    # a, b = [], []\n",
    "    # for idx in np.random.choice(len(A), size=(batch_size, 3)):\n",
    "    #     a.append(sum(A[idx]) % p)\n",
    "    #     b.append(sum(B[idx]) % p)\n",
    "    # a, b = np.array(a), np.array(b)\n",
    "    indices = np.random.choice(len(A), size=batch_size)\n",
    "    a, b = A[indices], B[indices]\n",
    "    pred = np.random.choice(p) # init guess\n",
    "    ll, grad = likelihood(pred, a, b, p)\n",
    "    t, lls, preds = 0, [ll], [pred]\n",
    "    best_result, min_loss = pred, ll\n",
    "    while t < p:\n",
    "        # use the reciprocal of the gradient, multiplied by the batch_size\n",
    "        pred -= lr * batch_size / grad\n",
    "        pred %= p\n",
    "        ll, grad = likelihood(pred, a, b, p)\n",
    "        lls.append(ll)\n",
    "        preds.append(pred % p)\n",
    "        if ll < min_loss:\n",
    "            min_loss, best_result = ll, pred\n",
    "        if verify(a, b, pred):\n",
    "            best_result = pred\n",
    "            # print(ll, ll/batch_size)\n",
    "            break\n",
    "        t += 1\n",
    "    return np.round(best_result % p, 5), preds, t\n",
    "\n",
    "def verify(a, b, pred):\n",
    "    # return np.abs(pred - s) < 0.5\n",
    "    err = ((a[:20]*np.round(pred))- b[:20]) % p\n",
    "    err[err > p//2] -= p\n",
    "    if np.std(err) < 6:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "np.random.seed(0)\n",
    "for batch_size in [64, 128, 256, 512]:\n",
    "    success = []\n",
    "    for lr in [0.5, 1, 2]:\n",
    "        for p in [251, 1471, 11197]: # 251, 1471, 11197, 130769\n",
    "            size = p-1 #int(np.sqrt(p))\n",
    "            a = np.random.choice([k for k in range(1,p)], size=size, replace=False)\n",
    "            steps = []\n",
    "            for s in np.random.choice([k for k in range(1,p)], 20, replace=False):\n",
    "                # print('s =', s)\n",
    "                b = (a*s) % p + np.random.normal(0, 3, size=size).astype(int)\n",
    "                starttime = time()\n",
    "                prediction, preds, t = circ_reg(p,a,b,lr, batch_size)\n",
    "        #         plt.plot(preds - s)\n",
    "        #         plt.show()\n",
    "        #         plt.close()\n",
    "                # print(f'p={p}, secret={s}, prediction={prediction}, time={np.round(time()-starttime, 2)}s, steps={t}')\n",
    "                if np.abs(prediction - s) < 0.5:\n",
    "                    steps.append(t)\n",
    "            success.append(len(steps))\n",
    "            if lr==2 and batch_size==256:\n",
    "                print(p, len(steps), sorted(steps))\n",
    "    print('/20 & '.join([str(t) for t in success]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b78c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3661c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "batch_size = 256\n",
    "lr = 2\n",
    "for p in [20663, 42899, 115301, 222553]: \n",
    "    size = p-1 \n",
    "    a = np.random.choice([k for k in range(1,p)], size=size, replace=False)\n",
    "    steps = []\n",
    "    for s in np.random.choice([k for k in range(1,p)], 20, replace=False):\n",
    "        b = (a*s) % p + np.random.normal(0, 3, size=size).astype(int)\n",
    "        starttime = time()\n",
    "        prediction, preds, t = circ_reg(p,a,b,lr, batch_size)\n",
    "        if np.abs(prediction - s) < 0.5:\n",
    "            steps.append(t)\n",
    "    success.append(len(steps))\n",
    "    print(p, lr, batch_size, len(steps), sorted(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9562e843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
